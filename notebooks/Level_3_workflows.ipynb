{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.models.huggingface import HuggingFace\n",
    "from agno.embedder.ollama import OllamaEmbedder\n",
    "from agno.models.ollama import Ollama\n",
    "from agno.agent import Agent\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=HuggingFace(\n",
    "        id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        api_key=os.getenv(\"HUGGINGFACE_API_TOKEN\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedder = OllamaEmbedder(id=\"nomic-embed-text:latest\",dimensions=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Searcher Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas Edison was an American inventor and businessman who developed many important technologies. He is best known for his invention of the light bulb, phonograph, and motion picture camera.\n"
     ]
    }
   ],
   "source": [
    "web_searcher = Agent(model=model, \n",
    "                     tools=[DuckDuckGoTools()], \n",
    "                     name=\"Web Searcher\",  \n",
    "                     role=\"Searches the web for information on a topic\", \n",
    "                     show_tool_calls=True,\n",
    "                     markdown=False)\n",
    "\n",
    "response = web_searcher.run(\"Who is Thomas Edison?\")\n",
    "print(response.content) #latencia 1min 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.vectordb.lancedb import LanceDb, SearchType\n",
    "\n",
    "# Define the database URL where the vector database will be stored\n",
    "db_url = \"./tmp/lancedb\"\n",
    "vector_db = LanceDb(\n",
    "    table_name=\"recipes\",  # Table name in the vector database\n",
    "    uri=db_url,  # Location to initiate/create the vector database\n",
    "    embedder=ollama_embedder,  # Without using this, it will use OpenAIChat embeddings by default\n",
    "    search_type=SearchType.vector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Creating collection                                                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Creating collection                                                                                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading knowledge base                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Loading knowledge base                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Reading: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf</span>                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Reading: \u001b[4;94mhttps://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\u001b[0m                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Added <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> documents to knowledge base                                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Added \u001b[1;36m0\u001b[0m documents to knowledge base                                                                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n",
    "# Create a knowledge base of PDFs from URLs\n",
    "knowledge_base = PDFUrlKnowledgeBase(\n",
    "    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n",
    "    vector_db=vector_db\n",
    ")\n",
    "# Load the knowledge base\n",
    "knowledge_base.load(recreate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and use the agent\n",
    "rag_agent = Agent(model=model,\n",
    "              name=\"RAG Agent\",\n",
    "              role=\"Responds to questions about Thai recipes\", \n",
    "              knowledge=knowledge_base,  \n",
    "\n",
    "              # Add a tool to search the knowledge base which enables agentic RAG.\n",
    "               # This is enabled by default when `knowledge` is provided to the Agent.\n",
    "              search_knowledge=True,\n",
    "              show_tool_calls=True,\n",
    "              markdown=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: To make chicken and galangal in coconut milk soup, you will need the following ingredients: 1 pound boneless, skinless chicken breast, 3 cloves of garlic, 1 tablespoon grated ginger, 1 tablespoon grated galangal, 2 cups coconut milk, 2 cups chicken broth, 1 tablespoon fish sauce, 1 tablespoon lime juice, 1 teaspoon palm sugar, 1/4 teaspoon turmeric, 1/4 teaspoon paprika, salt, and pepper. Instructions: 1. In a medium saucepan, heat the coconut milk and chicken broth over medium heat until it starts to simmer. 2. Add the chicken, garlic, ginger, galangal, fish sauce, lime juice, palm sugar, turmeric, paprika, salt, and pepper. Stir to combine. 3. Reduce the heat to low and let the soup simmer for 20-25 minutes or until the chicken is cooked through. 4. Serve the soup hot and garnish with fresh herbs like cilantro and lime wedges. You can adjust the level of spiciness to your liking by adding more or less chili peppers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = rag_agent.run(\"How do I make chicken and galangal in coconut milk soup\")\n",
    "print(\"response:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent, RunResponse\n",
    "from agno.workflow import Workflow\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "from typing import Optional, Iterator\n",
    "from agno.models.ollama import Ollama\n",
    "\n",
    "ollama_model=Ollama(id=\"llama3.2:3b\")\n",
    "\n",
    "class NewsArticle(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the article.\")\n",
    "    url: str = Field(..., description=\"Link to the article.\")\n",
    "    summary: Optional[str] = Field(..., description=\"Summary of the article if available.\")\n",
    "\n",
    "class SearchResults(BaseModel):\n",
    "    articles: list[NewsArticle]\n",
    "\n",
    "class BlogPostGenerator(Workflow):\n",
    "    # Define an Agent that will search the web for a topic\n",
    "    searcher_agent: Agent = Agent(\n",
    "        model=ollama_model,\n",
    "        tools=[DuckDuckGoTools()],\n",
    "        instructions=[\"Given a topic, search for the top 1 articles.\"],\n",
    "        response_model=SearchResults,\n",
    "        structured_outputs=True,\n",
    "    )\n",
    "\n",
    "    # Define an Agent that will write the blog post\n",
    "    writer_agent: Agent = Agent(\n",
    "        model=ollama_model,\n",
    "        instructions=[\n",
    "            \"You will be provided with a topic and a list of top articles on that topic.\",\n",
    "            \"Carefully read each article and generate a New York Times worthy blog post on that topic.\",\n",
    "            \"Break the blog post into sections and provide key takeaways at the end.\",\n",
    "            \"Make sure the title is catchy and engaging.\",\n",
    "            \"Always provide sources, do not make up information or sources.\",\n",
    "        ],\n",
    "        markdown=True,\n",
    "    )\n",
    "\n",
    "    def run(self, topic: str) -> Iterator[RunResponse]:\n",
    "        \"\"\"This is where the main logic of the workflow is implemented.\"\"\"\n",
    "\n",
    "        # Step 1: Search the web for articles on the topic\n",
    "        search_results: Optional[SearchResults] = self._get_search_results(topic)\n",
    "        if search_results is None:\n",
    "            anwser=f\"Sorry, could not find any articles on the topic: {topic}\",\n",
    "        else:\n",
    "            # Step 2: Write the blog post\n",
    "            anwser = self._write_blog_post(topic, search_results)\n",
    "            \n",
    "        return anwser\n",
    "                                                                           \n",
    "\n",
    "    def _get_search_results(self, topic: str) -> Optional[SearchResults]:\n",
    "        \"\"\"Get the search results for a topic.\"\"\"\n",
    "        print(\"Calling _get_search_results method\")\n",
    "        try:\n",
    "            searcher_response: RunResponse = self.searcher_agent.run(topic)\n",
    "            return searcher_response.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting search results for topic {topic}: {e}\")\n",
    "            return None\n",
    "     \n",
    "    def _write_blog_post(self,topic, search_results)-> Iterator[RunResponse]:\n",
    "        \"\"\"Write a blog post on a topic.\"\"\"\n",
    "        print(\"Calling _write_blog_post method\")\n",
    "        writer_input = {\"topic\": topic, \"articles\": [v.model_dump() for v in search_results.articles]}\n",
    "        # Run the writer and yield the response\n",
    "        return self.writer_agent.run(json.dumps(writer_input, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _get_search_results method\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> Error retrieving structured outputs: <span style=\"color: #008000; text-decoration-color: #008000\">'Message'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'parsed'</span>                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mWARNING \u001b[0m Error retrieving structured outputs: \u001b[32m'Message'\u001b[0m object has no attribute \u001b[32m'parsed'\u001b[0m                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _write_blog_post method\n"
     ]
    }
   ],
   "source": [
    "# flow = BlogPostGenerator()\n",
    "# response = flow.run(topic=\"What is artificial intelligence?\")\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnknownError",
     "evalue": "{'message': \"Input validation error: cannot compile regex from schema: 'type' must be a string\", 'http_status_code': 422}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m hn_team \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[0;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-Agent Team\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     markdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m \u001b[43mhn_team\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite an article about the top 2 stories on hackernews\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\agent\\agent.py:3351\u001b[0m, in \u001b[0;36mAgent.print_response\u001b[1;34m(self, message, messages, audio, images, videos, stream, markdown, show_message, show_reasoning, show_full_reasoning, console, tags_to_include_in_markdown, **kwargs)\u001b[0m\n\u001b[0;32m   3348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[0;32m   3349\u001b[0m     live_log\u001b[38;5;241m.\u001b[39mupdate(Group(\u001b[38;5;241m*\u001b[39mpanels))\n\u001b[1;32m-> 3351\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   3353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3354\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunResponse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRunEvent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_response\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\agent\\agent.py:534\u001b[0m, in \u001b[0;36mAgent._run\u001b[1;34m(self, message, stream, audio, images, videos, messages, stream_intermediate_steps, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m    533\u001b[0m     model_response \u001b[38;5;241m=\u001b[39m ModelResponse(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_messages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# If the model response is an assistant_response, yield a RunResponse with the content\u001b[39;49;00m\n\u001b[0;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mModelResponseEvent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massistant_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\models\\base.py:446\u001b[0m, in \u001b[0;36mModel.response_stream\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[0;32m    445\u001b[0m assistant_message\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mstart_timer()\n\u001b[1;32m--> 446\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response_stream(\n\u001b[0;32m    447\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages, assistant_message\u001b[38;5;241m=\u001b[39massistant_message, stream_data\u001b[38;5;241m=\u001b[39mstream_data\n\u001b[0;32m    448\u001b[0m )\n\u001b[0;32m    449\u001b[0m assistant_message\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mstop_timer()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# Populate assistant message from stream data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\models\\base.py:420\u001b[0m, in \u001b[0;36mModel.process_response_stream\u001b[1;34m(self, messages, assistant_message, stream_data)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_response_stream\u001b[39m(\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28mself\u001b[39m, messages: List[Message], assistant_message: Message, stream_data: MessageData\n\u001b[0;32m    416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ModelResponse]:\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    Process a streaming response from the model.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 420\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_provider_response_delta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_populate_stream_data_and_assistant_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massistant_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massistant_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response_delta\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\models\\huggingface\\huggingface.py:278\u001b[0m, in \u001b[0;36mHuggingFace.invoke_stream\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03mSend a streaming chat completion request to the HuggingFace API.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m    Iterator[ChatCompletionStreamOutput]: An iterator of chat completion delta.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_client()\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    279\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m    280\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[m\u001b[38;5;241m.\u001b[39mserialize_for_model() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    281\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    282\u001b[0m         stream_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_kwargs,\n\u001b[0;32m    284\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InferenceTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    286\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError invoking HuggingFace model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_common.py:327\u001b[0m, in \u001b[0;36m_stream_chat_completion_response\u001b[1;34m(bytes_lines)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m bytes_lines:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43m_format_chat_completion_stream_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_common.py:362\u001b[0m, in \u001b[0;36m_format_chat_completion_stream_output\u001b[1;34m(byte_payload)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;66;03m# Either an error as being returned\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _parse_text_generation_error(json_payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m], json_payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_type\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Or parse token payload\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatCompletionStreamOutput\u001b[38;5;241m.\u001b[39mparse_obj_as_instance(json_payload)\n",
      "\u001b[1;31mUnknownError\u001b[0m: {'message': \"Input validation error: cannot compile regex from schema: 'type' must be a string\", 'http_status_code': 422}"
     ]
    }
   ],
   "source": [
    "hn_team = Agent(\n",
    "    name=\"Multi-Agent Team\",\n",
    "    model=model,\n",
    "    team=[rag_agent, web_searcher],\n",
    "    instructions=[\n",
    "        \"First, search hackernews for what the user is asking about.\",\n",
    "        \"Then, ask the article reader to read the links for the stories to get more information.\",\n",
    "        \"Important: you must provide the article reader with the links to read.\",\n",
    "        \"Then, ask the web searcher to search for each story to get more information.\",\n",
    "        \"Finally, provide a thoughtful and engaging summary.\",\n",
    "    ],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,\n",
    ")\n",
    "hn_team.print_response(\"Write an article about the top 2 stories on hackernews\", stream=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
