{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.models.huggingface import HuggingFace\n",
    "from agno.embedder.ollama import OllamaEmbedder\n",
    "from agno.models.ollama import Ollama\n",
    "from agno.agent import Agent\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=HuggingFace(\n",
    "        id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        api_key=os.getenv(\"HUGGINGFACE_API_TOKEN\"),\n",
    ")\n",
    "\n",
    "\n",
    "# model=Ollama(id=\"llama3.2:3b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedder = OllamaEmbedder(id=\"nomic-embed-text:latest\",dimensions=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Searcher Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas Edison was an American inventor and businessman who developed many important technologies. He is best known for his invention of the light bulb, phonograph, and motion picture camera.\n"
     ]
    }
   ],
   "source": [
    "web_searcher = Agent(model=model, \n",
    "                     tools=[DuckDuckGoTools()], \n",
    "                     name=\"Web Searcher\",  \n",
    "                     role=\"Searches the web for information on a topic\", \n",
    "                     show_tool_calls=True,\n",
    "                     markdown=False)\n",
    "\n",
    "response = web_searcher.run(\"Who is Thomas Edison?\")\n",
    "print(response.content) #latencia 1min 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.vectordb.lancedb import LanceDb, SearchType\n",
    "\n",
    "# Define the database URL where the vector database will be stored\n",
    "db_url = \"./tmp/lancedb\"\n",
    "vector_db = LanceDb(\n",
    "    table_name=\"recipes\",  # Table name in the vector database\n",
    "    uri=db_url,  # Location to initiate/create the vector database\n",
    "    embedder=ollama_embedder,  # Without using this, it will use OpenAIChat embeddings by default\n",
    "    search_type=SearchType.vector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Creating collection                                                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Creating collection                                                                                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading knowledge base                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Loading knowledge base                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Reading: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf</span>                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Reading: \u001b[4;94mhttps://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\u001b[0m                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Added <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> documents to knowledge base                                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Added \u001b[1;36m0\u001b[0m documents to knowledge base                                                                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n",
    "# Create a knowledge base of PDFs from URLs\n",
    "knowledge_base = PDFUrlKnowledgeBase(\n",
    "    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n",
    "    vector_db=vector_db\n",
    ")\n",
    "# Load the knowledge base\n",
    "knowledge_base.load(recreate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and use the agent\n",
    "rag_agent = Agent(model=model,\n",
    "              name=\"RAG Agent\",\n",
    "              role=\"Responds to questions about Thai recipes\", \n",
    "              knowledge=knowledge_base,  \n",
    "\n",
    "              # Add a tool to search the knowledge base which enables agentic RAG.\n",
    "               # This is enabled by default when `knowledge` is provided to the Agent.\n",
    "              search_knowledge=False,\n",
    "              show_tool_calls=True,\n",
    "              markdown=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response = rag_agent.run(\"How do I make chicken and galangal in coconut milk soup\")\n",
    "# print(\"response:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_leader = Agent(\n",
    "    name=\"Multi-Agent Team\",\n",
    "    role=\"Team leader agent\",\n",
    "    model=model,\n",
    "    team=[web_searcher,rag_agent],\n",
    "     instructions=[\n",
    "        \"First, search  for what the user is asking about.\",\n",
    "        \"Finally, provide a thoughtful and engaging summary.\",\n",
    "    ],\n",
    "    show_tool_calls=True,\n",
    "    markdown=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions (Request ID: rnr_7Q)\n\nInput validation error: cannot compile regex from schema: 'type' must be a string\nMake sure 'text-generation' task is supported by the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mteam_leader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is Thomas Edison?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\agent\\agent.py:869\u001b[0m, in \u001b[0;36mAgent.run\u001b[1;34m(self, message, stream, audio, images, videos, messages, stream_intermediate_steps, retries, **kwargs)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m             resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\n\u001b[0;32m    860\u001b[0m                 message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m    861\u001b[0m                 stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    868\u001b[0m             )\n\u001b[1;32m--> 869\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ModelProviderError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    871\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_attempts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\agent\\agent.py:591\u001b[0m, in \u001b[0;36mAgent._run\u001b[1;34m(self, message, stream, audio, images, videos, messages, stream_intermediate_steps, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_run_response(\n\u001b[0;32m    586\u001b[0m                         content\u001b[38;5;241m=\u001b[39mmodel_response_chunk\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[0;32m    587\u001b[0m                         event\u001b[38;5;241m=\u001b[39mRunEvent\u001b[38;5;241m.\u001b[39mtool_call_completed,\n\u001b[0;32m    588\u001b[0m                     )\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;66;03m# Get the model response\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m     model_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_messages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;66;03m# Handle structured outputs\u001b[39;00m\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_outputs \u001b[38;5;129;01mand\u001b[39;00m model_response\u001b[38;5;241m.\u001b[39mparsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m         \u001b[38;5;66;03m# Update the run_response content with the structured output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\models\\base.py:163\u001b[0m, in \u001b[0;36mModel.response\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    159\u001b[0m model_response \u001b[38;5;241m=\u001b[39m ModelResponse()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Get response from model\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     assistant_message, has_tool_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_model_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Handle tool calls if present\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_tool_calls:\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;66;03m# Prepare function calls\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\models\\base.py:287\u001b[0m, in \u001b[0;36mModel._process_model_response\u001b[1;34m(self, messages, model_response)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[0;32m    286\u001b[0m assistant_message\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mstart_timer()\n\u001b[1;32m--> 287\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m assistant_message\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mstop_timer()\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Parse provider response\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\agno\\models\\huggingface\\huggingface.py:237\u001b[0m, in \u001b[0;36mHuggingFace.invoke\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03mSend a chat completion request to the HuggingFace Hub.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m    ChatCompletionOutput: The chat completion response from the Inference Client.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InferenceTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    243\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError invoking HuggingFace model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:970\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[0;32m    943\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    962\u001b[0m }\n\u001b[0;32m    963\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[0;32m    964\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    965\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    968\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    969\u001b[0m )\n\u001b[1;32m--> 970\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:327\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\GFT\\Projects\\Pilar\\vs_workspace\\repo\\pilarcode\\multiagents-lab\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions (Request ID: rnr_7Q)\n\nInput validation error: cannot compile regex from schema: 'type' must be a string\nMake sure 'text-generation' task is supported by the model."
     ]
    }
   ],
   "source": [
    "team_leader.run(\"Who is Thomas Edison?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
